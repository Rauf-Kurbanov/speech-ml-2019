{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaldi.feat.mfcc import Mfcc, MfccOptions\n",
    "from kaldi.matrix import SubVector, SubMatrix\n",
    "from kaldi.util.options import ParseOptions\n",
    "from kaldi.util.table import SequentialWaveReader\n",
    "from kaldi.util.table import MatrixWriter\n",
    "from numpy import mean\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Conv1D, Conv2D, Reshape, MaxPooling1D, Flatten, UpSampling1D\n",
    "from keras.layers import TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "scaler = MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "current_time = start_time\n",
    "def log(message):\n",
    "    global current_time\n",
    "    t = time()\n",
    "    print(f\"{message}, {int(t - current_time)}s, total {int(t - start_time)}s\")\n",
    "    current_time = t\n",
    "def start():\n",
    "    global start_time\n",
    "    global current_time\n",
    "    start_time = time()\n",
    "    current_time = start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"../data/mfcc.npy\", X_mfcc)\n",
    "# np.save(\"../data/mfcc_noised.npy\", X_mfcc_noised)\n",
    "X_mfcc = np.load(\"../data/mfcc.npy\")\n",
    "X_mfcc_noised = np.load(\"../data/mfcc_noised.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "X_mfcc_train, X_mfcc_test, X_mfcc_noised_train, X_mfcc_noised_test, y_train, y_test =\\\n",
    "    train_test_split(X_mfcc, X_mfcc_noised, y, test_size=test_size, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"../data/mfcc_train.npy\", X_mfcc_train)\n",
    "# np.save(\"../data/mfcc_test.npy\", X_mfcc_test)\n",
    "# np.save(\"../data/mfcc_noised_train.npy\", X_mfcc_noised_train)\n",
    "# np.save(\"../data/mfcc_noised_test.npy\", X_mfcc_noised_test)\n",
    "X_mfcc_noised_train = np.load(\"../data/mfcc_noised_train.npy\")\n",
    "X_mfcc_noised_test = np.load(\"../data/mfcc_noised_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"../data/y_train.npy\", y_train)\n",
    "# np.save(\"../data/y_test.npy\", y_test)\n",
    "y_train = np.load(\"../data/y_train.npy\")\n",
    "y_test = np.load(\"../data/y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Conv1DTranspose(input_tensor, filters, kernel_size, strides=2, padding='same', activation='relu'):\n",
    "#     x = keras.layers.Lambda(lambda x: keras.backend.expand_dims(x, axis=2))(input_tensor)\n",
    "#     x = Conv2DTranspose(filters=filters, kernel_size=(kernel_size, 1), strides=(strides, 1),\n",
    "#                         padding=padding, activation=activation)(x)\n",
    "#     x = keras.layers.Lambda(lambda x: keras.backend.squeeze(x, axis=2))(x)\n",
    "#     return x\n",
    "# def sampling(args):\n",
    "#     z_mean, z_log_sigma = args\n",
    "#     epsilon = keras.backend.random_normal(shape=(batch_size, latent_dim),\n",
    "#                               mean=0., stddev=1.)\n",
    "#     return z_mean + keras.backend.exp(z_log_sigma) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(X, Y):\n",
    "    for i in range(X.shape[0]):\n",
    "        x = MinMaxScaler().fit_transform(X[i])\n",
    "        x = x.reshape(1, *x.shape)\n",
    "        y = MinMaxScaler().fit_transform(Y[i])\n",
    "        y = y.reshape(1, *y.shape)\n",
    "        yield (x, y)\n",
    "def test_generator(X, k):\n",
    "    x = MinMaxScaler().fit_transform(X[k])\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_k = 32735\n",
    "# pred = rnn_ae.predict_generator(test_generator(X_mfcc_noised),\n",
    "# #                      epochs=10,\n",
    "#                      verbose=1,\n",
    "#                      steps=1)\n",
    "# true = MinMaxScaler().fit_transform(X_mfcc[test_k])\n",
    "# noisy = MinMaxScaler().fit_transform(X_mfcc_noised[test_k])\n",
    "# print(((pred - true)**2).mean())\n",
    "# print(((noisy - true)**2).mean()) \n",
    "# # ((pred.reshape(*pred.shape[1:]) - X_mfcc[0])**2).mean()\n",
    "# # r1 = keras.losses.mse(pred.reshape(*pred.shape[1:]),\n",
    "# #                                  tf.convert_to_tensor(X_mfcc[0]))\n",
    "# # r2 = keras.losses.mse(tf.convert_to_tensor(X_mfcc_noised[0]),\n",
    "#                                  tf.convert_to_tensor(X_mfcc[0]))\n",
    "def predict_ae(X):\n",
    "    res = []\n",
    "    for i in range(X.shape[0]):\n",
    "        if i % 500 == 0:\n",
    "            log(f\"{i}/{X.shape[0]}\")\n",
    "        pred = rnn_ae.predict_generator(test_generator(X, i),\n",
    "                                        verbose=0,\n",
    "                                        steps=1)\n",
    "        pred = pred.reshape(*pred.shape[1:])\n",
    "        res.append(pred)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_features_noised_train = np.load(\"../data/ae_features_noised_train.npy\")\n",
    "ae_features_noised_test = np.load(\"../data/ae_features_noised_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(set(y_train))\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mfcc_input = Input(shape=(None, 13), name='mfcc_input')\n",
    "gru1 = keras.layers.GRU(64,\n",
    "                        stateful=False,\n",
    "                        return_sequences=False)(mfcc_input)\n",
    "# aux_output = TimeDistributed(Dense(num_labels, activation='softmax', name='aux_output'))(gru1)\n",
    "aux_output = Dense(num_labels, activation='softmax', name='aux_output')(gru1)\n",
    "\n",
    "ae_input = Input(shape=(None, 12), name='ae_input')\n",
    "gru2 = keras.layers.GRU(20,\n",
    "                        stateful=False,\n",
    "                        return_sequences=False)(ae_input)\n",
    "\n",
    "x = keras.layers.concatenate([gru1, gru2])\n",
    "# x = TimeDistributed(Dense(64, activation='relu'))(x)\n",
    "# x = TimeDistributed(Dense(64, activation='relu'))(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# main_output = TimeDistributed(Dense(num_labels, activation='softmax', name='main_output'))(x)\n",
    "main_output = Dense(num_labels, activation='softmax', name='main_output')(x)\n",
    "\n",
    "model = Model(inputs=[mfcc_input, ae_input], outputs=[main_output, aux_output])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(X_main, X_aux, y):\n",
    "    for i in range(X_main.shape[0]):\n",
    "        mfcc = X_main[i]\n",
    "        mfcc = mfcc.reshape(1, *mfcc.shape)\n",
    "        ae = X_aux[i]\n",
    "        ae = ae.reshape(1, *ae.shape)\n",
    "#         local_y = np.array([y[i]] * mfcc.shape[1]).reshape(1, mfcc.shape[1], 1)\n",
    "#         print(np.array([y[i].todense()] * mfcc.shape[1]).shape)\n",
    "#         raise Exception\n",
    "#         local_y = np.array([y[i].todense()] * mfcc.shape[1]).reshape(1, mfcc.shape[1], num_labels)\n",
    "        local_y = y[i].todense()\n",
    "#         print(local_y.shape)\n",
    "        yield ({'mfcc_input': mfcc,\n",
    "                'ae_input': ae},\n",
    "               {'main_output': local_y, 'aux_output': local_y})\n",
    "def test_generator(X1, X2, k):\n",
    "    x1 = X1[k]\n",
    "    x1 = x1.reshape(1, *x1.shape)\n",
    "    x2 = X2[k]\n",
    "    x2 = x2.reshape(1, *x2.shape)\n",
    "    yield ({'mfcc_input': x1,\n",
    "            'ae_input': x2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mfcc_input (InputLayer)         (None, None, 13)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ae_input (InputLayer)           (None, None, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_20 (GRU)                    (None, 64)           14976       mfcc_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "gru_21 (GRU)                    (None, 20)           1980        ae_input[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 84)           0           gru_20[0][0]                     \n",
      "                                                                 gru_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 64)           5440        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 64)           4160        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 109)          7085        dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "aux_output (Dense)              (None, 109)          7085        gru_20[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 40,726\n",
      "Trainable params: 40,726\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365 365 (39817,)\n"
     ]
    }
   ],
   "source": [
    "print(len(X_mfcc_noised_train[0]), len(ae_features_noised_train[0]), y_train.shape)\n",
    "y_sparse_train = OneHotEncoder(categories='auto').fit_transform(y_train.reshape(-1, 1))\n",
    "y_sparse_test = OneHotEncoder(categories='auto').fit_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 67s 666ms/step - loss: 7.9924 - main_output_loss: 4.0623 - aux_output_loss: 3.9300\n",
      "\n",
      "Epoch 00001: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 73s 729ms/step - loss: 8.3397 - main_output_loss: 4.2271 - aux_output_loss: 4.1126\n",
      "\n",
      "Epoch 00002: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 74s 739ms/step - loss: 7.7978 - main_output_loss: 3.8996 - aux_output_loss: 3.8983\n",
      "\n",
      "Epoch 00003: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 68s 684ms/step - loss: 8.0886 - main_output_loss: 4.1287 - aux_output_loss: 3.9599\n",
      "\n",
      "Epoch 00004: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 68s 685ms/step - loss: 8.1165 - main_output_loss: 4.0908 - aux_output_loss: 4.0257\n",
      "\n",
      "Epoch 00005: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 72s 720ms/step - loss: 8.1457 - main_output_loss: 4.0937 - aux_output_loss: 4.0520\n",
      "\n",
      "Epoch 00006: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 70s 697ms/step - loss: 8.1210 - main_output_loss: 4.0544 - aux_output_loss: 4.0666\n",
      "\n",
      "Epoch 00007: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 73s 726ms/step - loss: 7.9741 - main_output_loss: 3.9770 - aux_output_loss: 3.9971\n",
      "\n",
      "Epoch 00008: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 70s 704ms/step - loss: 8.1299 - main_output_loss: 4.0542 - aux_output_loss: 4.0758\n",
      "\n",
      "Epoch 00009: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 69s 687ms/step - loss: 8.0290 - main_output_loss: 4.0694 - aux_output_loss: 3.9596\n",
      "\n",
      "Epoch 00010: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 70s 703ms/step - loss: 7.9240 - main_output_loss: 3.9917 - aux_output_loss: 3.9323\n",
      "\n",
      "Epoch 00011: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 69s 688ms/step - loss: 7.9370 - main_output_loss: 3.9859 - aux_output_loss: 3.9511\n",
      "\n",
      "Epoch 00012: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 69s 692ms/step - loss: 7.9394 - main_output_loss: 3.9356 - aux_output_loss: 4.0038\n",
      "\n",
      "Epoch 00013: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 72s 717ms/step - loss: 7.9893 - main_output_loss: 4.0242 - aux_output_loss: 3.9651\n",
      "\n",
      "Epoch 00014: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 69s 692ms/step - loss: 7.9899 - main_output_loss: 3.9674 - aux_output_loss: 4.0225\n",
      "\n",
      "Epoch 00015: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 67s 667ms/step - loss: 8.0203 - main_output_loss: 4.0532 - aux_output_loss: 3.9671\n",
      "\n",
      "Epoch 00016: saving model to ./models/classifyer.checkpoint\n",
      "Epoch 17/100\n",
      " 95/100 [===========================>..] - ETA: 3s - loss: 7.9463 - main_output_loss: 3.9380 - aux_output_loss: 4.0084"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-2a22feee585e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m           validation_steps=10)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/main_env/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main_env/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = keras.callbacks.ModelCheckpoint(\"./models/classifyer.checkpoint\", verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "model.fit_generator(train_generator(X_mfcc_noised_train[::-1], ae_features_noised_train[::-1], y_sparse_train[::-1]),\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "#           validation_data=({'mfcc_input': mfcc_seq_val, 'fbank_input': fbank_seq_val},\n",
    "#                            {'main_output': y_val_seq, 'aux_output': y_val_seq}),\n",
    "#           class_weight={0 : class_weights[0], 1 : class_weights[1]},\n",
    "#           initial_epoch=0,\n",
    "          callbacks = [saver],\n",
    "          steps_per_epoch=100,\n",
    "          validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X1, X2):\n",
    "    res = []\n",
    "    for i in range(X1.shape[0]):\n",
    "        if i % 500 == 0:\n",
    "            log(f\"{i}/{X1.shape[0]}\")\n",
    "        pred = model.predict_generator(test_generator(X1, X2, i),\n",
    "                                       verbose=0,\n",
    "                                       steps=1)\n",
    "#         pred = pred.reshape(*pred.shape[1:])\n",
    "        res.append(pred)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/4425, 0s, total 0s\n",
      "500/4425, 105s, total 105s\n",
      "1000/4425, 104s, total 209s\n",
      "1500/4425, 99s, total 308s\n",
      "2000/4425, 105s, total 413s\n",
      "2500/4425, 102s, total 516s\n",
      "3000/4425, 104s, total 620s\n",
      "3500/4425, 103s, total 723s\n",
      "4000/4425, 102s, total 825s\n"
     ]
    }
   ],
   "source": [
    "k = 500\n",
    "start()\n",
    "pred = predict(X_mfcc_noised_test, ae_features_noised_test)\n",
    "# pred = pred.reshape(*pred.shape[1:])\n",
    "# # np.argmax(.reshape(), axis=0)\n",
    "# res = np.argmax(pred) == y_test[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.array([np.argmax(x[1]) for x in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = (predicted_labels == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07209039548022599"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.sum() / res.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
